# Environment Variables Template
# Copy this file to .env and fill in your values

# ============================================================================
# LLM Provider Configuration
# ============================================================================
# Set LLM_PROVIDER to choose your AI backend (or leave as "auto" to auto-detect)
# Options: auto, groq, grok, gemini, ollama, openai
LLM_PROVIDER=auto

# Optional: Override the default model for your chosen provider
# LLM_MODEL=llama-3.3-70b-versatile

# ----------------------------------------------------------------------------
# FREE OPTIONS (Choose ONE):
# ----------------------------------------------------------------------------

# Option 1: Groq (RECOMMENDED - Fast & Free)
# Sign up at: https://console.groq.com/keys
# Free tier: Very generous, fast inference
GROQ_API_KEY=your_groq_api_key_here

# Option 2: Grok (xAI) - Free with X Premium
# Get key at: https://console.x.ai/
# Models: grok-beta, grok-2-1212
# XAI_API_KEY=your_xai_api_key_here

# Option 3: Google Gemini - Free tier available
# Get key at: https://makersuite.google.com/app/apikey
# Models: gemini-1.5-flash (fast), gemini-1.5-pro (powerful)
# GOOGLE_API_KEY=your_google_api_key_here

# Option 4: Ollama - Completely FREE, runs locally
# Install from: https://ollama.ai/
# Then run: ollama pull llama3.2
# No API key needed! Just set LLM_PROVIDER=ollama
# Models: llama3.2, mistral, codellama, phi3

# ----------------------------------------------------------------------------
# PAID OPTIONS:
# ----------------------------------------------------------------------------

# Option 5: OpenAI (Paid)
# OPENAI_API_KEY=your_openai_api_key_here

# ============================================================================
# Backend Configuration
# ============================================================================
LOG_LEVEL=INFO
MAX_UPLOAD_SIZE_MB=50
EXPORT_QUALITY=85

# ============================================================================
# Frontend Configuration (optional)
# ============================================================================
VITE_API_URL=http://localhost:8000
